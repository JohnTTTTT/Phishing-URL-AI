{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "13458df8-0910-4e9a-a977-6b7025c538f3",
    "_uuid": "a4c6d263-4fa6-47ea-9ab4-658f3aa1f7ae",
    "execution": {
     "iopub.execute_input": "2023-11-30T02:09:34.008632Z",
     "iopub.status.busy": "2023-11-30T02:09:34.008243Z",
     "iopub.status.idle": "2023-11-30T02:09:35.277330Z",
     "shell.execute_reply": "2023-11-30T02:09:35.276367Z",
     "shell.execute_reply.started": "2023-11-30T02:09:34.008602Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  status\n",
      "0                mp3raid.com/music/krizz_kaliko.html       1\n",
      "1                    bopsecrets.org/rexroth/cr/1.htm       1\n",
      "2  http://buzzfil.net/m/show-art/ils-etaient-loin...       1\n",
      "3      espn.go.com/nba/player/_/id/3457/brandon-rush       1\n",
      "4     yourbittorrent.com/?q=anthony-hamilton-soulife       1\n",
      "\n",
      "Shape: (188222, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "txtData = pd.read_csv('phishing_urls.csv')\n",
    "print(txtData.head())\n",
    "print(\"\\nShape:\", txtData.shape)\n",
    "\n",
    "urls = txtData['url']\n",
    "\n",
    "statuses = txtData['status']\n",
    "statuses = torch.tensor(statuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "ec9f78e4-d485-4657-b12a-8b90c54b01f9",
    "_uuid": "43ec1d11-b6c6-482b-9492-ed87525386ff",
    "execution": {
     "iopub.execute_input": "2023-11-30T02:09:49.583103Z",
     "iopub.status.busy": "2023-11-30T02:09:49.582417Z",
     "iopub.status.idle": "2023-11-30T02:13:07.225348Z",
     "shell.execute_reply": "2023-11-30T02:13:07.224292Z",
     "shell.execute_reply.started": "2023-11-30T02:09:49.583067Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dictionary = 'abcdefghijklmnopqrstuvwxyz_0123456789-;.!?:/\\\\|#$%^&~’+=<>(),\"’|^'\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.ToTensor()  # Convert url images to tensors \n",
    "    ])\n",
    "\n",
    "def one_hot_encode(text, dictionary, target_length):\n",
    "    encoded_text = []\n",
    "    for char in text:\n",
    "        one_hot = [int(char == c) for c in dictionary]\n",
    "        encoded_text.append(one_hot)\n",
    "        \n",
    "    encoded_array = np.array(encoded_text, dtype=np.uint8)\n",
    "    \n",
    "    # Pad or truncate the first dimension to the target length (256)\n",
    "    processed_array = np.pad(encoded_array, ((0, max(0, target_length - encoded_array.shape[0])), (0, 0)), mode='constant')[:, :target_length]\n",
    "    \n",
    "    # Clip the vectors to be at most 256 elements\n",
    "    processed_array = processed_array[:256, :]\n",
    "    \n",
    "    # Add channel and batch dimensions\n",
    "    processed_array = processed_array.reshape(1, 1, processed_array.shape[0], processed_array.shape[1])\n",
    "        \n",
    "    # Convert to pixel values (0 and 1) for visualization\n",
    "    pixel_values = (processed_array * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create a PIL Image\n",
    "    encoded_image = Image.fromarray(pixel_values[0, 0].squeeze(), mode='L')\n",
    "    \n",
    "    # Apply image transformation\n",
    "    encoded_image = image_transform(encoded_image)\n",
    "    \n",
    "    return encoded_image\n",
    "\n",
    "one_hot_urls = [one_hot_encode(url, dictionary, 256) for url in urls]\n",
    "\n",
    "url_set = [(url, status) for url, status in zip(one_hot_urls, statuses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "43d07a6b-2257-4268-a8d9-3453884b5772",
    "_uuid": "f9ea517d-449a-43dd-8d61-254b3815fe02",
    "execution": {
     "iopub.execute_input": "2023-11-30T02:14:00.672236Z",
     "iopub.status.busy": "2023-11-30T02:14:00.671874Z",
     "iopub.status.idle": "2023-11-30T02:14:00.726002Z",
     "shell.execute_reply": "2023-11-30T02:14:00.725057Z",
     "shell.execute_reply.started": "2023-11-30T02:14:00.672205Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL:  tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Status:  tensor(1)\n",
      "torch.Size([1, 256, 64])\n",
      "# of batches: 1250\n"
     ]
    }
   ],
   "source": [
    "print(\"URL: \", url_set[1][0])\n",
    "print(\"Status: \", url_set[1][1])\n",
    "print(url_set[0][0].shape)\n",
    "      \n",
    "batch_size = 128\n",
    "\n",
    "train_size = int(0.85 * len(url_set))\n",
    "test_size = int(0.05 * len(url_set))\n",
    "valid_size = len(url_set) - train_size - test_size\n",
    "\n",
    "train_set, test_set, valid_set = torch.utils.data.random_split(url_set, [train_size, test_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "print(\"# of batches:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "db41cb9a-dfba-48c4-9b62-7fa418b68f68",
    "_uuid": "6da646ff-9d84-4542-b804-ed7904a8f043",
    "execution": {
     "iopub.execute_input": "2023-11-30T02:14:06.840158Z",
     "iopub.status.busy": "2023-11-30T02:14:06.839825Z",
     "iopub.status.idle": "2023-11-30T02:14:06.851573Z",
     "shell.execute_reply": "2023-11-30T02:14:06.850544Z",
     "shell.execute_reply.started": "2023-11-30T02:14:06.840133Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder=None, classifier=None):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=(4, 4), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1),\n",
    "            nn.Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(5, 1),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32*26*2, 2),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.init_encoder_weights(mean=0.0, std=0.01)\n",
    "\n",
    "        self.init_classifier_weights(mean=0.0, std=0.01)\n",
    "\n",
    "    def init_encoder_weights(self, mean, std):\n",
    "        for param in self.encoder.parameters():\n",
    "            nn.init.normal_(param, mean=mean, std=std)\n",
    "\n",
    "    def init_classifier_weights(self, mean, std):\n",
    "        for param in self.classifier.parameters():\n",
    "            nn.init.normal_(param, mean=mean, std=std)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "#         print(\"After Transpose Shape:\", x.shape)\n",
    "        # print(x.shape)\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "#         print(\"Flattened Shape\", x.shape)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "915424d5-d5d3-483e-b301-9747deac72a8",
    "_uuid": "93ce8664-f32a-4823-873d-a21cf3024e38",
    "execution": {
     "iopub.execute_input": "2023-11-30T02:14:11.221127Z",
     "iopub.status.busy": "2023-11-30T02:14:11.220223Z",
     "iopub.status.idle": "2023-11-30T02:14:11.234950Z",
     "shell.execute_reply": "2023-11-30T02:14:11.234069Z",
     "shell.execute_reply.started": "2023-11-30T02:14:11.221091Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, scheduler, loss_fn, train_loader, device,\n",
    "          save_classifier_path, save_encoder_path, save_plot_path):\n",
    "    print(\"training...\")\n",
    "\n",
    "    avg_loss = []\n",
    "    losses_valid = []\n",
    "    epochs = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print('Epoch', epoch)\n",
    "\n",
    "        # Initialize a new list for this epoch\n",
    "        loss_train = 0.00\n",
    "\n",
    "        data_iter = iter(train_loader)\n",
    "\n",
    "        model.train()  # Keep track of gradient for backtracking\n",
    "\n",
    "        # Iterate through batches\n",
    "        for batch in range(int(len(train_loader))):\n",
    "            urls, labels = next(data_iter)\n",
    "            # Move tensors to the configured device\n",
    "            urls = urls.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass through model\n",
    "            # print(\"Input Shape Before Forward Pass:\", urls.shape)\n",
    "            outputs = model(urls)\n",
    "            \n",
    "            loss = loss_fn(outputs.squeeze(1), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        # Calculate the average loss over batches for the entire epoch\n",
    "        avg_loss += [loss_train / len(train_loader)]\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        loss_valid = 0.0\n",
    "        with torch.no_grad():\n",
    "            for urls, labels in valid_loader:\n",
    "                urls = urls.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(urls)\n",
    "                loss = loss_fn(outputs.squeeze(1), labels.float())\n",
    "                loss_valid += loss.item()\n",
    "\n",
    "        # Calculate the average validation loss for the entire epoch\n",
    "        avg_valid_loss = loss_valid / len(valid_loader)\n",
    "        losses_valid.append(avg_valid_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Arrays for plotting loss\n",
    "        epochs.append(epoch)\n",
    "\n",
    "        print('{} Epoch {}, Training loss {}, Validation loss {}'.format(datetime.datetime.now(), epoch,\n",
    "                                                                         loss_train / len(train_loader),\n",
    "                                                                         loss_valid / len(valid_loader)))\n",
    "\n",
    "    torch.save(model.classifier.state_dict(), save_classifier_path)\n",
    "    torch.save(model.encoder.state_dict(), save_encoder_path)\n",
    "\n",
    "    # Plot training and validation loss over epochs\n",
    "    plt.plot(epochs, avg_loss, label='Training Loss', color='blue')\n",
    "    plt.plot(epochs, losses_valid, label='Validation Loss', color='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig(save_plot_path)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "763f544b-d870-4f54-b20e-d938dc1198dd",
    "_uuid": "59c65bc8-99a2-4ac5-9ee2-f0b87219dbf2",
    "execution": {
     "iopub.execute_input": "2023-11-30T02:15:35.739508Z",
     "iopub.status.busy": "2023-11-30T02:15:35.739180Z",
     "iopub.status.idle": "2023-11-30T02:25:59.859765Z",
     "shell.execute_reply": "2023-11-30T02:25:59.858899Z",
     "shell.execute_reply.started": "2023-11-30T02:15:35.739484Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# batch size of 128 was used\n",
    "n_epochs = 60\n",
    "loss_fn = nn.BCELoss()\n",
    "save_classifier_path = 'CNNclassifier.pth'\n",
    "save_encoder_path = 'CNNencoder.pth'\n",
    "save_plot_path = 'loss.CNN.png'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = CNN()\n",
    "model.to(device)\n",
    "\n",
    "# summary(model, (1, 128, 16))\n",
    "\n",
    "# optimizer = optimizer = optim.SGD(model.parameters(), lr=0.0005, weight_decay=0.0005, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = ExponentialLR(optimizer=optimizer, gamma=0.9)\n",
    "\n",
    "train(n_epochs, optimizer, model, scheduler, loss_fn, train_loader, device,\n",
    "          save_classifier_path, save_encoder_path, save_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T02:32:53.286119Z",
     "iopub.status.busy": "2023-11-30T02:32:53.285734Z",
     "iopub.status.idle": "2023-11-30T02:32:53.748305Z",
     "shell.execute_reply": "2023-11-30T02:32:53.747509Z",
     "shell.execute_reply.started": "2023-11-30T02:32:53.286088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test(model, test_loader, device, loss_fn):\n",
    "    print(\"testing...\")\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    loss_test = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for urls, labels in test_loader:\n",
    "            urls = urls.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(urls)\n",
    "            outputs = outputs.squeeze(1)\n",
    "            loss = loss_fn(outputs, labels.float())\n",
    "            loss_test += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.round(outputs)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = loss_test / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    print('{} Test Loss: {}, Accuracy: {:.2%}'.format(datetime.datetime.now(), avg_test_loss, accuracy))\n",
    "    \n",
    "    return all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T02:32:57.715385Z",
     "iopub.status.busy": "2023-11-30T02:32:57.714791Z",
     "iopub.status.idle": "2023-11-30T02:32:58.114505Z",
     "shell.execute_reply": "2023-11-30T02:32:58.113620Z",
     "shell.execute_reply.started": "2023-11-30T02:32:57.715342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing...\n",
      "Confusion Matrix:\n",
      "[[4416  394]\n",
      " [ 178 4423]]\n",
      "2023-11-30 02:32:58.110859 Test Loss: 0.1669419023233491, Accuracy: 93.92%\n"
     ]
    }
   ],
   "source": [
    "encoder_path = '/kaggle/working/CNNencoder.pth'\n",
    "classifier_path = '/kaggle/working/CNNclassifier.pth'\n",
    "\n",
    "test_model = CNN()\n",
    "\n",
    "encoder_weights = torch.load(encoder_path, map_location=torch.device(device))   \n",
    "classifier_weights = torch.load(classifier_path, map_location=torch.device(device))\n",
    "\n",
    "test_model.to(device)\n",
    "\n",
    "test_model.encoder.load_state_dict(encoder_weights)\n",
    "test_model.classifier.load_state_dict(classifier_weights)\n",
    "\n",
    "preds, labels = test(test_model, test_loader, device, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-30T02:30:46.271163Z",
     "iopub.status.busy": "2023-11-30T02:30:46.270768Z",
     "iopub.status.idle": "2023-11-30T02:30:46.276990Z",
     "shell.execute_reply": "2023-11-30T02:30:46.275938Z",
     "shell.execute_reply.started": "2023-11-30T02:30:46.271134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.0 0\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.0 0\n",
      "0.0 0\n",
      "1.0 1\n",
      "1.0 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(preds[i], labels[i])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4081467,
     "sourceId": 7084198,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4083066,
     "sourceId": 7086628,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
